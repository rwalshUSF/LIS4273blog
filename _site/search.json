[
  {
    "objectID": "posts/Welcome/index.html",
    "href": "posts/Welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in my LIS4273 blog. Welcome!"
  },
  {
    "objectID": "posts/Module 8 Assignment/index.html",
    "href": "posts/Module 8 Assignment/index.html",
    "title": "Module 8 Assignment",
    "section": "",
    "text": "An Exercise in ANOVA, analysis of variance!\n\n# LIS4273 - Module 8. Assignment\n# Robert Walsh\n# Professor Lingyao Li\n\n# Question A.\n\n# A researcher is interested in the effects of drug against\n# stress reactions. She gives a reaction time test to three\n# different groups of subjects:\n# one group that is under a great deal of stress,\n# one group under a moderate amount of stress,\n# and a third group that is under almost no stress.\n# The subjects of the study were instructed to take the drug test\n# during their next stress episode and to report their stress on a\n# scale of 1 to 10 (10 being the most pain).\n\ndf&lt;-data.frame(\"High.Stress\"=c(10,9,8,9,10,8),\n               \"Moderate.Stress\"=c(8,10,6,7,8,8),\n               \"Low.Stress\"=c(4,6,6,4,2,2))\ndf\n\n  High.Stress Moderate.Stress Low.Stress\n1          10               8          4\n2           9              10          6\n3           8               6          6\n4           9               7          4\n5          10               8          2\n6           8               8          2\n\nstress.pain.level&lt;-c(df$High.Stress,df$Moderate.Stress,df$Low.Stress)\n\nsubject.group.amount&lt;-factor(c(rep(\"High\",6),\n                        rep(\"Moderate\",6),\n                        rep(\"Low\",6)),\n                        levels = c(\"High\",\"Moderate\",\"Low\"))\n\n# Report on drug and stress level by using R.\n# Provide a full summary report on the result\n# of ANOVA testing and what it means.\n\n# Look at the means for stress level by group:\ntapply(stress.pain.level,subject.group.amount,mean)\n\n    High Moderate      Low \n9.000000 7.833333 4.000000 \n\n# Look at the variations for stress level by group:\ntapply(stress.pain.level,subject.group.amount,var)\n\n    High Moderate      Low \n0.800000 1.766667 3.200000 \n\n# Look at a boxplot graph:\nboxplot(stress.pain.level ~ subject.group.amount,\n        main = \"Effects of Drug Against Stress Reactions\",\n        xlab = \"Stress amount groups\",\n        ylab = \"Stress level pain (1-10)\")\n\n\n\n\n\n\n\n# More specifically, report the following:\n# Df, Sum, Sq Mean, Sq, F value, Pr(&gt;F)\n\n# Did the drug have any affect on specific groups stress levels?\n\n# Test the difference between the means of the groups with ANOVA.\n\n# Null Ho: The mean stress levels are similar between groups\n# Alt  Ha: The mean stress levels are significantly different between groups\naov.out&lt;-aov(stress.pain.level ~ subject.group.amount,data = df)\nsummary(aov.out)\n\n                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsubject.group.amount  2  82.11   41.06   21.36 4.08e-05 ***\nResiduals            15  28.83    1.92                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# The Df for subject.group.amount is '2' because there are three\n# groups sampled, so k-1 = 2, where k = number of groups sampled.\n# for the residuals row, the Df is '15' because there are 6 observations\n# in 3 samples, so N-k = 18-3 = 15.\n\n# Sum Sq is for 'Sum of the Squares' which is measuring the variance,\n# the higher this number is = more variance in the data, or groups.\n# Sum Sq for subject.group.amount, SS(B) = 82.11, indicating a large\n# variance between the groups, and Sum Sq for the residuals, SS(W) = 28.83\n# indicating a large variance within the groups as well.\n# Adding both of these would give the Total Variance.\n# SS(B)+SS(W)=SS(T)\n\n# Mean Sq normalizes the Sums of Squares by calculating mean squares.\n# Size of obtained mean differences (including treatment effects) = MS(B)\n# The Mean Sq for 'subject.group.amount' is: MS(B)=SS(B)/(K-1) = 41.06,\n# There is a large difference between the means in the sample groups.\n# Size of expected differences by chance (without treatment effects) = MS(W)\n# The Mean Sq for the residuals is: MS(W)=SS(W)/(N-k) = 1.92\n# The residuals row shows a smaller difference within the sample groups.\n\n# The F value is the result of MS(B)/MS(W) = 21.36\n# This test accounts for the random variation causing difference\n# in the mean squares and is the analysis of variance between means.\n# A value close to '1' indicates that the treatment has little to no effect\n# on the group means, which does not signify a difference between groups,\n# and there is not enough evidence to reject the Null Hypothesis that the\n# means are very similar or identical.\n# A large F value indicates a significant treatment effect, and would \n# cause the Null Hypothesis of similar means to be rejected because there\n# is enough evidence for the Alternative Hypothesis to be considered.\n\n# The Pr(&gt;F) from ANOVA works like similar p-value significance tests.\n# 95% confidence needs a p-value less than 0.05 to reject the Null.\n# In this test the p-value = 4.08e-05, is significantly smaller than 0.001,\n# indicating that the statistics are very significant.\n\n# The large F value at 21.36 and the low 4.08e-05 p-value suggests\n# evidence against the null hypothesis. Therefore, it can be assumed\n# that the mean stress levels differ significantly among the three groups.\n\n# Question B.\n\n# From our Textbook: Introductory Statistics with R.\n# Chapter # 6 Exercises 6.1 pp. 127 plus.\n# The zelazo data (taken from the textbook's R package called ISwR)\n# are in the form of a list of vectors, one for each of the four groups.\n\n# B1. Convert the data to a form suitable for the user of lm,\n# and calculate the relevant test.\nlibrary(ISwR)\ndata(\"zelazo\")\nsummary(zelazo)\n\n        Length Class  Mode   \nactive  6      -none- numeric\npassive 6      -none- numeric\nnone    6      -none- numeric\nctr.8w  5      -none- numeric\n\n# To convert the data for linear model, lm()\n# A 'value' column will hold the combined response variables\n# A 'group' factor will create the categorical variable that\n# will be used to combine the predictor variables\nzelazo.df&lt;-data.frame(\nvalue=unlist(zelazo,use.names = F),\ngroup=factor(rep(c(\"active\",\"passive\",\"none\",\"ctr.8w\"),sapply(zelazo,length)),\nlevels = c(\"active\",\"passive\",\"none\",\"ctr.8w\")))\nzelazo.df\n\n   value   group\n1   9.00  active\n2   9.50  active\n3   9.75  active\n4  10.00  active\n5  13.00  active\n6   9.50  active\n7  11.00 passive\n8  10.00 passive\n9  10.00 passive\n10 11.75 passive\n11 10.50 passive\n12 15.00 passive\n13 11.50    none\n14 12.00    none\n15  9.00    none\n16 11.50    none\n17 13.25    none\n18 13.00    none\n19 13.25  ctr.8w\n20 11.50  ctr.8w\n21 12.00  ctr.8w\n22 13.50  ctr.8w\n23 11.50  ctr.8w\n\n# Boxplot to view age at walking values by four group levels\nboxplot(zelazo.df$value~zelazo.df$group,\n        main = \"Effects of Training on Infant Walking Ages\",\n        xlab = \"Training Level Groups\",\n        ylab = \"Walking Age in Months\")\n\n\n\n\n\n\n\n# Model and call the linear model, lm()\n# This will model the relationship between the magnitude of one\n# variable and that of a second\nzelazo.lm&lt;-lm(zelazo.df$value~zelazo.df$group)\nsummary(zelazo.lm)\n\n\nCall:\nlm(formula = zelazo.df$value ~ zelazo.df$group)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7083 -0.8500 -0.3500  0.6375  3.6250 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             10.1250     0.6191  16.355 1.19e-12 ***\nzelazo.df$grouppassive   1.2500     0.8755   1.428   0.1696    \nzelazo.df$groupnone      1.5833     0.8755   1.809   0.0864 .  \nzelazo.df$groupctr.8w    2.2250     0.9182   2.423   0.0255 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.516 on 19 degrees of freedom\nMultiple R-squared:  0.2528,    Adjusted R-squared:  0.1348 \nF-statistic: 2.142 on 3 and 19 DF,  p-value: 0.1285\n\n# The residual standard error of 1.516 could be considered small\n# indicating that the regression model fits the data set well.\n# The median value in the distribution of the residuals is close to zero\n# at -0.3500 and the min & max are roughly equal in absolute value.\n# An F-value of 2.14 with a p-value of 0.1285 indicates that there is not\n# enough evidence to reject the null hypothesis that the model with no\n# independent variables fits the data as well as the model including treatment\n# effects.The observed differences between groups are likely due to chance,\n# as the p-value is relatively high and not statistically significant at the\n# standard 0.05 significance level. To find the relationship between groups\n# that is causing the difference, further investigation is necessary.\n\n# Consider t-tests comparing selected subgroups\n# or obtained by combining groups.\n\n# The only group in the regression summary that showed significant\n# statistical results was the 'ctr.8w' control group with a t-value\n# of 2.423 and a Pr(&gt;|t|) value of 0.0255 * indicating significant evidence\n# to reject the null that the true coefficient of the control group and the\n# intercept is equal to zero, and a relationship between the variables exists.\n\n# So I will begin by using t-tests to compare the subgroups of the \n# control group and the other groups to test for significant differences\n# between the means of the groups.\ntapply(zelazo.df$value,zelazo.df$group,var)\n\n  active  passive     none   ctr.8w \n2.093750 3.593750 2.310417 0.925000 \n\n# The variances are not the same between the groups so the Welch's\n# variant of the t-test is sufficient.\n\n# Two sample t-test comparing the means of 'none' group to 'control' group\nt.test.result1&lt;-t.test(zelazo.df$value~zelazo.df$group,\ndata=zelazo.df,\nsubset=group%in%c(\"none\",\"ctr.8w\"))\nt.test.result1\n\n\n    Welch Two Sample t-test\n\ndata:  zelazo.df$value by zelazo.df$group\nt = -0.84986, df = 8.5046, p-value = 0.4187\nalternative hypothesis: true difference in means between group none and group ctr.8w is not equal to 0\n95 percent confidence interval:\n -2.364947  1.081614\nsample estimates:\n  mean in group none mean in group ctr.8w \n            11.70833             12.35000 \n\n# The p-value of 0.4187 is &gt; 0.05 alpha, so there is not significant evidence\n# to reject the null hypothesis that there is no difference between the means\n\n# Two sample t-test comparing the means of 'passive' group to 'control' group\nt.test.result2&lt;-t.test(zelazo.df$value~zelazo.df$group,\ndata=zelazo.df,\nsubset=group%in%c(\"passive\",\"ctr.8w\"))\nt.test.result2\n\n\n    Welch Two Sample t-test\n\ndata:  zelazo.df$value by zelazo.df$group\nt = -1.1012, df = 7.6531, p-value = 0.3042\nalternative hypothesis: true difference in means between group passive and group ctr.8w is not equal to 0\n95 percent confidence interval:\n -3.032993  1.082993\nsample estimates:\nmean in group passive  mean in group ctr.8w \n               11.375                12.350 \n\n# The p-value of 0.3042 is &gt; 0.05 alpha, so there is not significant evidence\n# to reject the null hypothesis that there is no difference between the means\n\n# Two sample t-test comparing the means of active group to control group\nt.test.result3&lt;-t.test(zelazo.df$value~zelazo.df$group,\ndata=zelazo.df,\nsubset=group%in%c(\"active\",\"ctr.8w\"))\nt.test.result3\n\n\n    Welch Two Sample t-test\n\ndata:  zelazo.df$value by zelazo.df$group\nt = -3.0449, df = 8.6632, p-value = 0.01453\nalternative hypothesis: true difference in means between group active and group ctr.8w is not equal to 0\n95 percent confidence interval:\n -3.8878619 -0.5621381\nsample estimates:\nmean in group active mean in group ctr.8w \n              10.125               12.350 \n\n# The p-value of 0.01453 is &lt; 0.05 alpha, so there is significant evidence\n# to reject the null hypothesis that there is no difference between the means!\n# We can conclude that a difference between these two groups' means exists!\n\n\n\n# B2. Consider the ANOVA test (one-way or two-way) for this dataset (zelazo)\n\n# The one-way ANOVA test will test the null hypothesis that there is no\n# difference between the means of the group that received the treatment and \n# the means of the groups that did not receive the treatment.\n\n# h0: All group means are equal\n# ha: All group means are not equal\n\n# The one-way test to compare differences between the group means\noneway.test(zelazo.df$value~zelazo.df$group)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  zelazo.df$value and zelazo.df$group\nF = 2.7759, num df = 3.000, denom df = 10.506, p-value = 0.09373\n\n# With an F value = 2.7759, and p-value = 0.09373, we fail to reject the null\n# hypothesis that all group means are equal. This is strange because we know\n# they are different somehow, further analysis of the groups is needed.\n\n# The aov() function fits analysis of variance (ANOVA)\n# models directly to the data\nzelazo.aov&lt;-aov(zelazo.df$value~zelazo.df$group,data=zelazo.df)\nzelazo.aov\n\nCall:\n   aov(formula = zelazo.df$value ~ zelazo.df$group, data = zelazo.df)\n\nTerms:\n                zelazo.df$group Residuals\nSum of Squares         14.77781  43.68958\nDeg. of Freedom               3        19\n\nResidual standard error: 1.516394\nEstimated effects may be unbalanced\n\nsummary(zelazo.aov)\n\n                Df Sum Sq Mean Sq F value Pr(&gt;F)\nzelazo.df$group  3  14.78   4.926   2.142  0.129\nResiduals       19  43.69   2.299               \n\n# It returns an ANOVA table with sources of variation and associated statistics\n\n# The anova() function performs analysis of variance (ANOVA)\n# on model objects\nanova(zelazo.lm)\n\nAnalysis of Variance Table\n\nResponse: zelazo.df$value\n                Df Sum Sq Mean Sq F value Pr(&gt;F)\nzelazo.df$group  3 14.778  4.9259  2.1422 0.1285\nResiduals       19 43.690  2.2995               \n\n# It returns an ANOVA table comparing models or factors,\n# showing sources of variation and statistics\n\n# Which yields the same results as the lm() \nsummary(zelazo.lm)\n\n\nCall:\nlm(formula = zelazo.df$value ~ zelazo.df$group)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7083 -0.8500 -0.3500  0.6375  3.6250 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             10.1250     0.6191  16.355 1.19e-12 ***\nzelazo.df$grouppassive   1.2500     0.8755   1.428   0.1696    \nzelazo.df$groupnone      1.5833     0.8755   1.809   0.0864 .  \nzelazo.df$groupctr.8w    2.2250     0.9182   2.423   0.0255 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.516 on 19 degrees of freedom\nMultiple R-squared:  0.2528,    Adjusted R-squared:  0.1348 \nF-statistic: 2.142 on 3 and 19 DF,  p-value: 0.1285\n\nplot(zelazo.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Because the p-value is &gt; 0.05, we fail to reject the null hypothesis\n# that each group mean is equal, However, Post-Hoc Tests can be\n# used to explore how the groups differ from each other to learn more\n# about where the variance in the means exists.\n\n# The Tukey Test offers a glimpse at the relationships between all of the\n# pairs and combinations of the groups\nTukeyHSD(zelazo.aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = zelazo.df$value ~ zelazo.df$group, data = zelazo.df)\n\n$`zelazo.df$group`\n                    diff        lwr      upr     p adj\npassive-active 1.2500000 -1.2117450 3.711745 0.4982812\nnone-active    1.5833333 -0.8784116 4.045078 0.3000594\nctr.8w-active  2.2250000 -0.3568999 4.806900 0.1063883\nnone-passive   0.3333333 -2.1284116 2.795078 0.9806495\nctr.8w-passive 0.9750000 -1.6068999 3.556900 0.7160591\nctr.8w-none    0.6416667 -1.9402332 3.223567 0.8962600\n\n# The adjusted p-value will indicate if there is a significant\n# statistical relationship between two groups.\n\n# The ctr.8w-active is closest to being less than 0.05 at 0.1063883,\n# but is still not considered significant enough to be outside of the 95%\n# confidence level"
  },
  {
    "objectID": "posts/Module 6 Assignment/index.html",
    "href": "posts/Module 6 Assignment/index.html",
    "title": "Module 6 Assignment",
    "section": "",
    "text": "Random Variables & Probability Distributions!\n\n# LIS4273 - Module 6. Assignment\n# Robert Walsh\n# Professor Lingyao Li\n\n# Question A.\n\n# Consider a population consisting of the following values,\n# which represents the number of ice cream purchases during\n# one year for each of the five housemates.\n\n# 8, 14, 16, 10, 11.\nnumPurchases&lt;-c(8,14,16,10,11)\n# A1. Compute the mean of this population.\npopMean&lt;-mean(numPurchases)\npopMean\n\n[1] 11.8\n\n# A2. Select a random sample of size 2 out of the five members.\nrSample&lt;-sample(numPurchases,2)\nrSample\n\n[1] 16  8\n\n# A3. Compute the mean and standard deviation of your sample.\n\n# The Mean of the sample:\nrSampleMean&lt;-mean(rSample)\nrSampleMean\n\n[1] 12\n\n# The standard deviation of the sample:\nrSampleStdDev&lt;-sd(rSample)\nrSampleStdDev\n\n[1] 5.656854\n\n# A4. Compare the Mean and Standard deviation of your sample\n# to the entire population of this set (8,14, 16, 10, 11).\n\n# The standard deviation of the population:\npopStdDev&lt;-sd(numPurchases)\npopStdDev\n\n[1] 3.193744\n\ndf&lt;-data.frame(\"Population\"=c(popMean,popStdDev),\n               \"Sample\"=c(rSampleMean,rSampleStdDev),\n               row.names=(c(\"Mean\",\"Std Dev\")))\n# The data frame to compare the Mean and Standard deviation of the sample\n# to the entire population of this set (8,14, 16, 10, 11).\ndf\n\n        Population    Sample\nMean     11.800000 12.000000\nStd Dev   3.193744  5.656854\n\n# Question B. \n\n# Suppose that the sample size n = 100 and\n# the population proportion p = 0.95.\n\n# B1. Does the sample proportion p have approximately\n# a normal distribution? Explain.\ncat(\"The distribution is considered normal if both :\n      n*p &gt;= 5, and\n      n*q &gt;= 5.\n      n = 100\n      p = 0.95 and \n      q = 0.05.\n      \n      Therefore,\n      n*p = 100*0.95 = 95 &gt;= 5 and\n      n*q = 100*0.05 = 5 &gt;= 5.\n      \n      The sample proportion 'p' will have approximately\n      a normal distribution with these values.\")\n\nThe distribution is considered normal if both :\n      n*p &gt;= 5, and\n      n*q &gt;= 5.\n      n = 100\n      p = 0.95 and \n      q = 0.05.\n      \n      Therefore,\n      n*p = 100*0.95 = 95 &gt;= 5 and\n      n*q = 100*0.05 = 5 &gt;= 5.\n      \n      The sample proportion 'p' will have approximately\n      a normal distribution with these values.\n\n# B2. What is the smallest value of n for which the\n# sampling distribution of p is approximately normal?\ncat(\"The smallest value of 'n' for which the sampling distribution\n      of 'p' is approximately normal is 100.\n      \n      Any value less than 100, when multiplied with p = 0.95\n      and q = 0.05 will make n*q &lt;= 5,\n      and both n*p and n*q need to be &gt;= 5\n      in order to assume a normal distribution.\")\n\nThe smallest value of 'n' for which the sampling distribution\n      of 'p' is approximately normal is 100.\n      \n      Any value less than 100, when multiplied with p = 0.95\n      and q = 0.05 will make n*q &lt;= 5,\n      and both n*p and n*q need to be &gt;= 5\n      in order to assume a normal distribution.\n\n# Question C.\n# From our textbook, Chapter 2 Probability Exercises # 2.4.\n# Simulated coin tossing is probability better done using\n# function called 'rbinom()' than using the function called 'sample()'.\n\n# C1. Please explain the reason why 'rbinom()' is better\n# than 'sample()' in the coin tossing simulation.\ncat(\"The reason why the 'rbinom()' function is better\n      than the 'sample()' function in the coin tossing\n      simulation is that the 'sample()' function would need to\n      create a vector with 'Char' type elements like \\\"H\\\"\n      & \\\"T\\\", then set 'replace=TRUE', & set'prob=c(0.5,0.5)'.\n      That would require extra programming instructions\n      in order to simulate random sampling from that vector.\n      This could affect the outcomes / experiment results. \n      This extra step can be avoided by using 'rbinom()'\n      because it is designed to extract random sample outcomes from\n      the binomial distribution. This makes 'rbinom()' the better\n      choice because it is more efficient and readable as the code \n      clearly indicates that a binomial event, like a coin toss, is \n      being simulated.\"\n      )\n\nThe reason why the 'rbinom()' function is better\n      than the 'sample()' function in the coin tossing\n      simulation is that the 'sample()' function would need to\n      create a vector with 'Char' type elements like \"H\"\n      & \"T\", then set 'replace=TRUE', & set'prob=c(0.5,0.5)'.\n      That would require extra programming instructions\n      in order to simulate random sampling from that vector.\n      This could affect the outcomes / experiment results. \n      This extra step can be avoided by using 'rbinom()'\n      because it is designed to extract random sample outcomes from\n      the binomial distribution. This makes 'rbinom()' the better\n      choice because it is more efficient and readable as the code \n      clearly indicates that a binomial event, like a coin toss, is \n      being simulated."
  },
  {
    "objectID": "posts/Module 4 Assignment/index.html",
    "href": "posts/Module 4 Assignment/index.html",
    "title": "Module 4 Assignment",
    "section": "",
    "text": "An exercise in probability!\n\n# LIS4273 - Module 4. Assignment\n# Robert Walsh\n# Professor Lingyao Li\n#\ntable1&lt;-data.frame(\"B\"=c(10,20,30),\"B1\"=c(20,40,60),\"Totals\"=c(30,60,90),\nrow.names = c(\"A\",\"A1\",\"Totals\"))\nlibrary(gridExtra)\ngrid.table(table1)\n\n\n\n\n\n\n\n#Question A.\n#Based on Table 1, what is the probability of:\n#A1. Event A ?\nA&lt;-30/90\npaste(\"The probability of event A = \",format(round(A,4)),\"%\")\n\n[1] \"The probability of event A =  0.3333 %\"\n\n#A2. Event B ?\nB&lt;-30/90\npaste(\"The probability of event B = \",format(round(B,4)),\"%\")\n\n[1] \"The probability of event B =  0.3333 %\"\n\n#A3. Event A or B ?\nAorB&lt;-((30/90)+(30/90))-(10/90)\npaste(\"The probability of event A or B = \",format(round(AorB,4)),\"%\")\n\n[1] \"The probability of event A or B =  0.5556 %\"\n\n#A4. P(A or B) = P(A) + P(B)\nAorB == A + B\n\n[1] FALSE\n\n# Question B.\n# In terms of probabilities, we know the following:\n# P( A1 ) = 5/365 =0.0136985 [It rains 5 days out of the year.]\n# P( A2 ) = 360/365 = 0.9863014 [It does not rain 360 days out of the year.]\n# P( B | A1 ) = 0.9 [When it rains, the weatherman predicts rain 90% of the time.]\n# P( B | A2 ) = 0.1 [When it does not rain, the weatherman predicts rain 10% of the time.]\n# We want to know P( A1 | B ), the probability it will rain on the day of Jane's wedding,\n# given a forecast for rain by the weatherman. \n\n# The answer can be determined from Bayes' theorem, as shown below.\n\n# P( A1 | B ) = P( A1 ) P( B | A1 ) / P( A1 ) P( B | A1 ) + P( A2 ) P( B | A2 )\n# P( A1 | B ) = (0.014)(0.9) / [ (0.014)(0.9) + (0.986)(0.1) ]\n# P( A1 | B ) = 0.111\n# Note the somewhat unintuitive result. Even when the weatherman predicts rain,\n# it only rains only about 11% of the time. \n# Despite the weatherman's gloomy prediction, there is a good chance that Jane\n# will not get rained on at her wedding.\n\n# Please answer the following questions:\n# B1. Is this answer True or False ?\nprint(\"TRUE\")\n\n[1] \"TRUE\"\n\n# B2. Please explain why:\ncat(\"Bayes' Theorem takes into account your initial belief about the event\n(like the usual chance of rain for the day) before new information is considered. \nEven though the weatherman correctly predicts rain 90% of the time..\nThe base probability of rain is already low, 5/365 = 1.36%.\nEven if the weatherman predicts rain,\nthe new information isn't strong enough to significantly update the probability\nhigh enough to consider it a \\\"high chance\\\" of rain.\nThe \\\"likelihood ratio\\\" is not large enough\nto drastically change the initial low probability of rain.\")\n\nBayes' Theorem takes into account your initial belief about the event\n(like the usual chance of rain for the day) before new information is considered. \nEven though the weatherman correctly predicts rain 90% of the time..\nThe base probability of rain is already low, 5/365 = 1.36%.\nEven if the weatherman predicts rain,\nthe new information isn't strong enough to significantly update the probability\nhigh enough to consider it a \"high chance\" of rain.\nThe \"likelihood ratio\" is not large enough\nto drastically change the initial low probability of rain.\n\n# Question C.\n# Last assignment from our textbook, pp. 55 Exercise # 2.3.\n# For a disease known to have a postoperative complication frequency of 20%,\n# a surgeon suggests a new procedure. She/he tests it on 10 patients and found\n# there are not complications.\n\n# C1.You will answer this question with the following code.\n# What is the probability of operating on 10 patients successfully\n# with the traditional method?\npaste(\"The probability is:\",dbinom(x=0,size=10,prob=.20),\"%\")\n\n[1] \"The probability is: 0.1073741824 %\""
  },
  {
    "objectID": "posts/Module 2 Assignment/index.html",
    "href": "posts/Module 2 Assignment/index.html",
    "title": "Module 2 Assignment",
    "section": "",
    "text": "This Function Calculates The Mean!\nHere is how it works!\n\n# LIS4273 - Module 2. Assignment\n# Robert Walsh\n# Professor Lingyao Li\n\n# Create a vector using the c() function\n# and store the results in the 'assignment2' variable/object\n\nassignment2 &lt;- c(6,18,14,22,27,17,22,20,22)\n\n# Create a function named 'myMean' that will\n# accept 'assignment2' as an input variable/argument and then\n# calculate & return the value of the sum of all values in the\n# 'assignment2' vector using the sum() function, and then divide\n# that result by the number of components in the 'assignment2'\n# vector using the length() function. the result is stored in the\n# 'result' variable/object.\n\nmyMean &lt;- function(assignment2) \n{\n  return(sum(assignment2)/length(assignment2))\n}\n\n# print the result as output by calling 'myMean' with \n# 'assignment2' as the argument\n\nresult &lt;- myMean(assignment2)\nresult\n\n[1] 18.66667\n\n# Here is the output!"
  },
  {
    "objectID": "posts/Module 11 Assignment/index.html",
    "href": "posts/Module 11 Assignment/index.html",
    "title": "Module 11 Assignment",
    "section": "",
    "text": "Exercises in GLM, Matrices, and Logistic Regression!\n\n# LIS4273 - Module 11. Assignment\n# Robert Walsh\n# Professor Lingyao Li\n\n# Question A.\n\n# Set up an additive model for the ashina data, as part of ISwR package\nlibrary(ISwR)\ndata(\"ashina\")\nattach(ashina)\n# This data contain additive effects on subjects, period and treatment.\n# Compare the results with those obtained from t tests. \nhead(ashina,5)\n\n  vas.active vas.plac grp\n1       -167     -102   1\n2       -127      -39   1\n3        -58       32   1\n4       -103       28   1\n5        -35       16   1\n\n# Create a new 'ashina$subject' column as a factor w/ values 1-16\nashina$subject&lt;-factor(1:16)\nhead(ashina,5)\n\n  vas.active vas.plac grp subject\n1       -167     -102   1       1\n2       -127      -39   1       2\n3        -58       32   1       3\n4       -103       28   1       4\n5        -35       16   1       5\n\nsummary(ashina)\n\n   vas.active         vas.plac            grp           subject  \n Min.   :-167.00   Min.   :-102.00   Min.   :1.000   1      : 1  \n 1st Qu.: -81.25   1st Qu.: -36.75   1st Qu.:1.000   2      : 1  \n Median : -51.50   Median :  -5.00   Median :1.000   3      : 1  \n Mean   : -56.81   Mean   : -13.94   Mean   :1.375   4      : 1  \n 3rd Qu.: -14.25   3rd Qu.:  11.25   3rd Qu.:2.000   5      : 1  \n Max.   :  29.00   Max.   :  32.00   Max.   :2.000   6      : 1  \n                                                     (Other):10  \n\n# Create two data frames:\n\n# 'act' Given active substance & 'plac' given Placebo\n# Include a 'vas' column for the 'vas' summary score value,\n# a 'subject' column for the factored numeric subject count,\n# a 'treat' column (1 for 'active' & 0 for 'placebo')\n# to distinguish between treatment in binary format,\n# and a 'period' column to identify the group number from 'ashina$grp'\n# where '1' got placebo first & '2' got active first.\n\nattach(ashina)\nact&lt;-data.frame(vas=vas.active,subject,treat=1,period=grp)\nplac&lt;-data.frame(vas=vas.plac,subject,treat=0,period=grp)\n\n# In order to fit an additive model, the 'act' & 'plac' data.frames must be\n# combined together using the rbind() function and the binary values\n# converted to factors as well as the subject count.\n\ncombinedData&lt;-rbind(act,plac)\ncombinedData$treat&lt;-factor(combinedData$treat,\n                           labels=c(\"placebo\",\"active\"))\ncombinedData$period&lt;-factor(combinedData$period,\n                           labels=c(\"placFirst\",\"actFirst\"))\nsummary(combinedData)\n\n      vas             subject       treat          period  \n Min.   :-167.00   1      : 2   placebo:16   placFirst:20  \n 1st Qu.: -58.75   2      : 2   active :16   actFirst :12  \n Median : -32.50   3      : 2                              \n Mean   : -35.38   4      : 2                              \n 3rd Qu.:   4.25   5      : 2                              \n Max.   :  32.00   6      : 2                              \n                   (Other):20                              \n\n# Now the data frame can be modeled to compare the predictor variables \n# to the response variable to see if there is a significant statistical\n# difference to 'var' values from the additive effects of:\n# 'subject'+'treat' to see if the placebo or active treatments\n# had a significant effect on vas scores among the 16 subjects.\n\n# Plot the result\nattach(combinedData)\nplot(as.numeric(subject),vas,pch=as.numeric(treat),\n     xlab=\"Subject #\", ylab=\"vas score\",col=treat)\nlegend(x=1,legend=c(\"placebo\",\"active\"),\n       col=c(\"black\",\"red\"),pch=1:2)\naxis(1, at = c(1:16))\n\n# Specify regression line data\nvas.placebo&lt;-combinedData[treat==\"placebo\",]\nvas.active&lt;-combinedData[treat==\"active\",]\n\n# Fit GLM models for effects of treatment on vas scores by subject\nglm.placebo&lt;-glm(vas~as.numeric(subject),data=vas.placebo)\nglm.active&lt;-glm(vas~as.numeric(subject),data=vas.active)\nabline(glm.placebo,col=\"black\")\nabline(glm.active,col=\"red\")\n\n\n\n\n\n\n\n# Regression analysis\nsummary(glm.placebo)\n\n\nCall:\nglm(formula = vas ~ as.numeric(subject), data = vas.placebo)\n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)          -28.475     18.383  -1.549    0.144\nas.numeric(subject)    1.710      1.901   0.900    0.384\n\n(Dispersion parameter for gaussian family taken to be 1228.886)\n\n    Null deviance: 18199  on 15  degrees of freedom\nResidual deviance: 17204  on 14  degrees of freedom\nAIC: 163.09\n\nNumber of Fisher Scoring iterations: 2\n\n# The results show that with a t-value of 0.900 and a \n# p-value of 0.384, the placebo treatment did not have a significant\n# enough effect on vas scores among the 16 subjects to reject the null.\nsummary(glm.active)\n\n\nCall:\nglm(formula = vas ~ as.numeric(subject), data = vas.active)\n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)         -111.375     28.070  -3.968   0.0014 **\nas.numeric(subject)    6.419      2.903   2.211   0.0442 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2865.194)\n\n    Null deviance: 54122  on 15  degrees of freedom\nResidual deviance: 40113  on 14  degrees of freedom\nAIC: 176.64\n\nNumber of Fisher Scoring iterations: 2\n\n# The results show that with a t-value of 2.211 and a \n# p-value of 0.0442, the active treatment had a significant\n# effect on vas scores among the 16 subjects. We can reject the null.\n\n# Create 'logit' function for logistic regression\nlogit  &lt;- function(p) log(p/(1-p))\n\n# GLM Model to determine if 'vas, subject, or period' predictor variables\n# increases or decreases the odds of the outcome 'treat' group\nmodel &lt;- glm(treat ~ .,family = binomial(logit), data=combinedData)\nsummary(model)\n\n\nCall:\nglm(formula = treat ~ ., family = binomial(logit), data = combinedData)\n\nCoefficients: (1 not defined because of singularities)\n               Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)    -8.08937    3.50126  -2.310  0.02087 * \nvas            -0.06014    0.02057  -2.923  0.00346 **\nsubject2        3.09742    3.71629   0.833  0.40458   \nsubject3        7.30749    4.40133   1.660  0.09686 . \nsubject4        5.83397    5.94041   0.982  0.32606   \nsubject5        7.51800    3.82633   1.965  0.04944 * \nsubject6        1.89454    5.06324   0.374  0.70827   \nsubject7        7.18721    3.59417   2.000  0.04553 * \nsubject8        7.93901    3.96059   2.005  0.04502 * \nsubject9        4.84159    3.07156   1.576  0.11497   \nsubject10       6.97670    3.72291   1.874  0.06093 . \nsubject11       7.30749    3.76965   1.939  0.05256 . \nsubject12       9.29225    4.10554   2.263  0.02361 * \nsubject13       7.87886    3.73104   2.112  0.03471 * \nsubject14       7.51800    3.65361   2.058  0.03962 * \nsubject15       5.95426    3.85548   1.544  0.12250   \nsubject16       4.84159    3.16071   1.532  0.12557   \nperiodactFirst       NA         NA      NA       NA   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 44.361  on 31  degrees of freedom\nResidual deviance: 28.281  on 15  degrees of freedom\nAIC: 62.281\n\nNumber of Fisher Scoring iterations: 5\n\n# Use AIC to fit model selection\nmodel2 &lt;- step(model,data=combinedData)\n\nStart:  AIC=62.28\ntreat ~ vas + subject + period\n\n\nStep:  AIC=62.28\ntreat ~ vas + subject\n\n          Df Deviance    AIC\n- subject 15   38.342 42.342\n&lt;none&gt;         28.281 62.281\n- vas      1   44.361 76.361\n\nStep:  AIC=42.34\ntreat ~ vas\n\n       Df Deviance    AIC\n&lt;none&gt;      38.342 42.342\n- vas   1   44.361 46.361\n\nsummary(model2)\n\n\nCall:\nglm(formula = treat ~ vas, family = binomial(logit), data = combinedData)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -0.639381   0.471332  -1.357   0.1749  \nvas         -0.019715   0.009389  -2.100   0.0357 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 44.361  on 31  degrees of freedom\nResidual deviance: 38.342  on 30  degrees of freedom\nAIC: 42.342\n\nNumber of Fisher Scoring iterations: 4\n\n# The results show that the estimates (log odds) -0.019715 are negative for 'vas'\n# A negative log-odds ratio means that the odds go down with an\n# increase in the value of the predictor variable.\n# Therefore the odds of determining a treatment group 'treat' decreases\n# with observed/summarized 'vas' score increases. \n# A Z-value of -2.100 and a p value of 0.0357, suggest that we can reject\n# the null, and that 'vas' has a significant effect on 'treat'.\n\n# Perform t test\n\ntTest1&lt;-t.test(vas~treat,data=combinedData)\ntTest1\n\n\n    Welch Two Sample t-test\n\ndata:  vas by treat\nt = 2.4699, df = 24.063, p-value = 0.02099\nalternative hypothesis: true difference in means between group placebo and group active is not equal to 0\n95 percent confidence interval:\n  7.052494 78.697506\nsample estimates:\nmean in group placebo  mean in group active \n             -13.9375              -56.8125 \n\n# When testing to determine if 'treat' affects 'vas'\n# The t test results state that the t-statistic is 2.4699\n# with 24.063 degrees of freedom. Because the p-value 0.02099 is less than\n# alpha 0.05 the null hypothesis would be rejected.This concludes that both\n# treatments have a significant effect on the 'vas' scores with the active\n# treatment group showing lower mean 'vas' score compared to the placebo group.\n# The p-value is similar to the p-value from the model summary which\n# indicated that 'treat' was statistically significant.\n\n\n#---------------------------------------------------------------------------#\n\n\n# Question B\n\n# Consider the following\na&lt;-gl(2,2,8) # Creates factor with 2 levels,each repeated 2 times,length 8\nb&lt;-gl(2,4,8) # Creates factor with 2 levels,each repeated 4 times,length 8\nx&lt;-1:8\ny&lt;-c(1:4,8:5)\nz&lt;-rnorm(8)\n\n# Instruction: The rnorm() is a built-in R function that generates a vector\n# of normally distributed random numbers. The rnorm() method takes a sample\n# size as input and generates that many random numbers. We are looking for\n# two models: (1) model.matrix (z~a:b); (2) lm (z~a:b)\n\n# B1. Your assignment is to generate the model matrices for the following models\n\n# z ~ a*b  # Model with interaction (a*b)\n\nmm1&lt;-model.matrix(z~a*b)\nmm1\n\n  (Intercept) a2 b2 a2:b2\n1           1  0  0     0\n2           1  0  0     0\n3           1  1  0     0\n4           1  1  0     0\n5           1  0  1     0\n6           1  0  1     0\n7           1  1  1     1\n8           1  1  1     1\nattr(,\"assign\")\n[1] 0 1 2 3\nattr(,\"contrasts\")\nattr(,\"contrasts\")$a\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$b\n[1] \"contr.treatment\"\n\nfmm1&lt;-lm(z~a*b)\nsummary(fmm1)\n\n\nCall:\nlm(formula = z ~ a * b)\n\nResiduals:\n      1       2       3       4       5       6       7       8 \n-0.2766  0.2766 -0.3711  0.3711 -0.6979  0.6979 -0.8791  0.8791 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.35116    0.60703  -0.578    0.594\na2           0.14326    0.85847   0.167    0.876\nb2           0.05227    0.85847   0.061    0.954\na2:b2        0.16001    1.21407   0.132    0.902\n\nResidual standard error: 0.8585 on 4 degrees of freedom\nMultiple R-squared:  0.04765,   Adjusted R-squared:  -0.6666 \nF-statistic: 0.06671 on 3 and 4 DF,  p-value: 0.9747\n\n# z ~ a:b  # Model with only interaction term (a:b)\n\nmm2&lt;-model.matrix(z~a:b)\nmm2\n\n  (Intercept) a1:b1 a2:b1 a1:b2 a2:b2\n1           1     1     0     0     0\n2           1     1     0     0     0\n3           1     0     1     0     0\n4           1     0     1     0     0\n5           1     0     0     1     0\n6           1     0     0     1     0\n7           1     0     0     0     1\n8           1     0     0     0     1\nattr(,\"assign\")\n[1] 0 1 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$a\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$b\n[1] \"contr.treatment\"\n\nfmm2&lt;-lm(z~a:b)\nsummary(fmm2)\n\n\nCall:\nlm(formula = z ~ a:b)\n\nResiduals:\n      1       2       3       4       5       6       7       8 \n-0.2766  0.2766 -0.3711  0.3711 -0.6979  0.6979 -0.8791  0.8791 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.004386   0.607033   0.007    0.995\na1:b1       -0.355542   0.858475  -0.414    0.700\na2:b1       -0.212279   0.858475  -0.247    0.817\na1:b2       -0.303276   0.858475  -0.353    0.742\na2:b2              NA         NA      NA       NA\n\nResidual standard error: 0.8585 on 4 degrees of freedom\nMultiple R-squared:  0.04765,   Adjusted R-squared:  -0.6666 \nF-statistic: 0.06671 on 3 and 4 DF,  p-value: 0.9747\n\n# B2. Please also discuss the implications of using these two models;\n# please be reminded about the model fits and notice\n# which models contain singularities.\n\n# Interaction Model Matrix\nmm1\n\n  (Intercept) a2 b2 a2:b2\n1           1  0  0     0\n2           1  0  0     0\n3           1  1  0     0\n4           1  1  0     0\n5           1  0  1     0\n6           1  0  1     0\n7           1  1  1     1\n8           1  1  1     1\nattr(,\"assign\")\n[1] 0 1 2 3\nattr(,\"contrasts\")\nattr(,\"contrasts\")$a\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$b\n[1] \"contr.treatment\"\n\n# This design matrix will include columns for the main effects of a and b,\n# as well as their interaction term (a:b).\n\n# Interaction-term-Only Model Matrix\nmm2\n\n  (Intercept) a1:b1 a2:b1 a1:b2 a2:b2\n1           1     1     0     0     0\n2           1     1     0     0     0\n3           1     0     1     0     0\n4           1     0     1     0     0\n5           1     0     0     1     0\n6           1     0     0     1     0\n7           1     0     0     0     1\n8           1     0     0     0     1\nattr(,\"assign\")\n[1] 0 1 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$a\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$b\n[1] \"contr.treatment\"\n\n# This design matrix will only include the interaction term (a:b),\n# without main effects. \n\n# Interaction Model Fit\nsummary(fmm1)\n\n\nCall:\nlm(formula = z ~ a * b)\n\nResiduals:\n      1       2       3       4       5       6       7       8 \n-0.2766  0.2766 -0.3711  0.3711 -0.6979  0.6979 -0.8791  0.8791 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.35116    0.60703  -0.578    0.594\na2           0.14326    0.85847   0.167    0.876\nb2           0.05227    0.85847   0.061    0.954\na2:b2        0.16001    1.21407   0.132    0.902\n\nResidual standard error: 0.8585 on 4 degrees of freedom\nMultiple R-squared:  0.04765,   Adjusted R-squared:  -0.6666 \nF-statistic: 0.06671 on 3 and 4 DF,  p-value: 0.9747\n\n# This model will estimate the effects of a, b,\n# and their interaction on the response variable z\n\n# Interaction-term-Only Model Fit\nsummary(fmm2)\n\n\nCall:\nlm(formula = z ~ a:b)\n\nResiduals:\n      1       2       3       4       5       6       7       8 \n-0.2766  0.2766 -0.3711  0.3711 -0.6979  0.6979 -0.8791  0.8791 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.004386   0.607033   0.007    0.995\na1:b1       -0.355542   0.858475  -0.414    0.700\na2:b1       -0.212279   0.858475  -0.247    0.817\na1:b2       -0.303276   0.858475  -0.353    0.742\na2:b2              NA         NA      NA       NA\n\nResidual standard error: 0.8585 on 4 degrees of freedom\nMultiple R-squared:  0.04765,   Adjusted R-squared:  -0.6666 \nF-statistic: 0.06671 on 3 and 4 DF,  p-value: 0.9747\n\n# This model will estimate the effect of the interaction term a:b on z,\n# assuming no separate effects for a or b.\n# A singularity occurs here, probably because the main effects of a and b \n# are highly correlated with the interaction term, and it doesn't account \n# for the separate effects.\n\n# Both models have the same R-squared value, indicating similar\n# explanatory power. However, the significance levels of the coefficients\n# differ between the two models. In the interaction model, you can assess \n# the significance of the main effects and the interaction term separately.\n# In the interaction-term-only model, you can only assess the significance of \n# the interaction term."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " USF - LIS4273 Advanced Stats - Blog ",
    "section": "",
    "text": "Module 12 Assignment\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule 11 Assignment\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule 10 Assignment\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule 9 Assignment\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule 8 Assignment\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule 7 Assignment\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule 6 Assignment\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule 5 Assignment\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule 4 Assignment\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule 3 Assignment\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule 2 Assignment\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Robert Walsh",
    "section": "",
    "text": "UNIVERSITY OF SOUTH FLORIDA - SCHOOL OF INFORMATION - LIS4273\nA blog for sharing my class assignments."
  },
  {
    "objectID": "posts/Module 10 Assignment/index.html",
    "href": "posts/Module 10 Assignment/index.html",
    "title": "Module 10 Assignment",
    "section": "",
    "text": "An Exercise in Multivariate Regression!\n\n# LIS4273 - Module 10. Assignment\n# Robert Walsh\n# Professor Lingyao Li\n\n# Question A.\n\n# Conduct ANOVA (analysis of variance) and Regression coefficients to \n# the data from data (\" cystfibr \") database. You can choose any variable \n# you like to interpret.\nlibrary(ISwR)\ndata(\"cystfibr\")\nattach(cystfibr)\n\n#-------------------------------------------------------------------------#\n# A1. In the report, please state the result of coefficients and \n# significance to any variables you like both under ANOVA and multivariate \n# analysis. Please provide a specific interpretation of R results.\n\n# model:\n# 'bmp' as a function of 'age'+'height'+'weight'+'rv'+'frc'+'tlc'+'pemax'\nbmpModelOne&lt;-lm(bmp~age+height+weight+rv+frc+tlc+pemax,data=cystfibr)\n# The Coefficients:\nbmpModelOne\n\n\nCall:\nlm(formula = bmp ~ age + height + weight + rv + frc + tlc + pemax, \n    data = cystfibr)\n\nCoefficients:\n(Intercept)          age       height       weight           rv          frc  \n  138.94756     -2.34798     -0.34820      1.42015     -0.02505      0.02401  \n        tlc        pemax  \n   -0.15536     -0.06993  \n\n# The intercept is 138.94756 \n# Therefore, the estimated body mass percentage of normal would be\n# around 139 if all predictor variables were 0.\n\n# age: indicates that for each increment of 1 in age, a person’s 'bmp' \n# decreases by 2.34798 with all other variables constant.\n\n# height: indicates that for each increment of 1 in height, a person’s 'bmp' \n# decreases by 0.34820 with all other variables constant.\n\n# weight: indicates that for each increment of 1 in weight, a person’s 'bmp' \n# increases by 1.42015 with all other variables constant.\n\n# Summary of the regression model to seek significant relationships\n# among the variables:\nsummary(bmpModelOne)\n\n\nCall:\nlm(formula = bmp ~ age + height + weight + rv + frc + tlc + pemax, \n    data = cystfibr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8666 -1.8780 -0.6527  2.4338 11.2400 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 138.94756   18.76512   7.405 1.03e-06 ***\nage          -2.34798    0.62819  -3.738  0.00164 ** \nheight       -0.34820    0.14754  -2.360  0.03049 *  \nweight        1.42015    0.18965   7.488 8.87e-07 ***\nrv           -0.02505    0.03586  -0.699  0.49425    \nfrc           0.02401    0.07976   0.301  0.76705    \ntlc          -0.15536    0.08926  -1.741  0.09983 .  \npemax        -0.06993    0.04332  -1.615  0.12481    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.997 on 17 degrees of freedom\nMultiple R-squared:  0.8773,    Adjusted R-squared:  0.8267 \nF-statistic: 17.36 on 7 and 17 DF,  p-value: 1.357e-06\n\n# This analysis suggests that 'age', 'height', and 'weight' are significant\n# to 'bmp', and that 'rv', 'frc', 'tlc', and 'pemax' could be removed from\n# the model without significantly affecting the dependent variable outcomes &\n# the F-statistic of 17.36 with a p-value of 1.357e-06 suggests that at least\n# one predictor variable is significantly related to the response variable.\n\n#---------------------------------------------------------------------------#\n# ANOVA of the regression model to seek significant relationships\n# among the variables:\nanova(bmpModelOne)\n\nAnalysis of Variance Table\n\nResponse: bmp\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nage        1  493.63  493.63 19.7681 0.0003543 ***\nheight     1  200.82  200.82  8.0421 0.0114097 *  \nweight     1 2020.55 2020.55 80.9164 7.138e-08 ***\nrv         1  130.04  130.04  5.2078 0.0356384 *  \nfrc        1    0.01    0.01  0.0002 0.9875888    \ntlc        1  124.40  124.40  4.9819 0.0393593 *  \npemax      1   65.09   65.09  2.6068 0.1248148    \nResiduals 17  424.50   24.97                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# This analysis suggests that 'age','height','weight','rv', and 'tlc' are \n# significant to 'bmp' and 'frc' & 'pemax' could be removed from\n# the model without significantly affecting the dependent variable outcomes.\n# The ANOVA table indicates that there is no significant improvement of the\n# model once 'age','height','weight','rv', and 'tlc' are included.\n\n# Perform a joint test to determine if 'frc' & 'pemax' could be removed from\n# the model without significantly affecting the dependent variable outcomes.\nbmpModelTwo&lt;-lm(bmp~age+height+weight+rv+tlc,data=cystfibr)\nanova(bmpModelOne,bmpModelTwo)\n\nAnalysis of Variance Table\n\nModel 1: bmp ~ age + height + weight + rv + frc + tlc + pemax\nModel 2: bmp ~ age + height + weight + rv + tlc\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     17 424.50                           \n2     19 512.47 -2   -87.967 1.7614 0.2017\n\n# The ANOVA table suggests we can remove 'frc' & 'pemax'.\n# The large p-value 0.2017 provides good evidence that 'model 2' with five\n# predictors fits as well as 'model 1' with seven predictors.\n\n#---------------------------------------------------------------------------#\n\n# Dropping 'pemax' from the analysis may result in losing valuable data\n# Therefore, a Multivariate Multiple Regression will model multiple dependent\n# variables, with a single set of predictor variables.\n\n# model:\n# 'bmp' and 'pemax' as a function of 'age'+'height'+'weight'+'rv'+'frc'+'tlc'\nmlm1&lt;-lm(cbind(cystfibr$bmp,cystfibr$pemax)~age+height+weight+rv+frc+tlc)\n# Summary of the Multivariate Multiple Regression model to seek significant \n# relationships among the variables:\nsummary(mlm1)\n\nResponse cystfibr$bmp :\n\nCall:\nlm(formula = `cystfibr$bmp` ~ age + height + weight + rv + frc + \n    tlc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.8520 -2.8217 -0.0276  2.5375 10.5757 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 135.57876   19.46330   6.966 1.66e-06 ***\nage          -2.32637    0.65548  -3.549  0.00229 ** \nheight       -0.33877    0.15387  -2.202  0.04097 *  \nweight        1.31722    0.18642   7.066 1.37e-06 ***\nrv           -0.04520    0.03508  -1.288  0.21393    \nfrc           0.07106    0.07749   0.917  0.37125    \ntlc          -0.19249    0.09001  -2.139  0.04643 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 18 degrees of freedom\nMultiple R-squared:  0.8585,    Adjusted R-squared:  0.8113 \nF-statistic:  18.2 on 6 and 18 DF,  p-value: 9.551e-07\n\n\nResponse cystfibr$pemax :\n\nCall:\nlm(formula = `cystfibr$pemax` ~ age + height + weight + rv + \n    frc + tlc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.447 -19.051  -0.572  16.175  41.161 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  48.1705   101.4776   0.475    0.641\nage          -0.3090     3.4175  -0.090    0.929\nheight       -0.1349     0.8022  -0.168    0.868\nweight        1.4719     0.9719   1.514    0.147\nrv            0.2881     0.1829   1.575    0.133\nfrc          -0.6728     0.4040  -1.665    0.113\ntlc           0.5310     0.4693   1.132    0.273\n\nResidual standard error: 27.19 on 18 degrees of freedom\nMultiple R-squared:  0.504, Adjusted R-squared:  0.3387 \nF-statistic: 3.048 on 6 and 18 DF,  p-value: 0.03083\n\n# Is a predictor variable jointly contributing to both models?\n\n# The model for Response 'cystfibr$bmp' suggests that at least one \n# predictor variable is significantly related to the response variable.\n# F-statistic:  18.2 & p-value: 9.551e-07\n# 'age','height','weight',and 'tlc' are significant\n\n# The model for Response 'cystfibr$pemax' suggests that at least one \n# predictor variable is significantly related to the response variable.\n# F-statistic: 3.048 & p-value: 0.03083\n# No predictors indicate significance, the p-value: 0.03083 does.\n\n# Determining whether or not to include predictors in a multivariate multiple\n# regression requires the use of multivariate test statistics (Ford, 2024)\n# The easiest way to do this is to use the Anova() function\n# in the car package (Fox and Weisberg, 2011)\nlibrary(car)\nAnova(mlm1)\n\n\nType II MANOVA Tests: Pillai test statistic\n       Df test stat approx F num Df den Df    Pr(&gt;F)    \nage     1   0.45134    6.992      2     17  0.006083 ** \nheight  1   0.24796    2.803      2     17  0.088726 .  \nweight  1   0.79365   32.692      2     17 1.493e-06 ***\nrv      1   0.14568    1.449      2     17  0.262289    \nfrc     1   0.13809    1.362      2     17  0.282776    \ntlc     1   0.20762    2.227      2     17  0.138338    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# The Type II MANOVA Tests suggest 'height','rv','frc',and 'tlc' are\n# jointly insignificant for the two models\n\n# Check to see if a model with just 'age' and 'weight' fits as well as \n# a model with all six predictors.\n# Fit a smaller model and then compare the smaller model to the larger\n# model using the anova() function (Ford, 2024)\nmlm2&lt;-lm(cbind(cystfibr$bmp,cystfibr$pemax)~age+weight)\nanova(mlm1,mlm2)\n\nAnalysis of Variance Table\n\nModel 1: cbind(cystfibr$bmp, cystfibr$pemax) ~ age + height + weight + \n    rv + frc + tlc\nModel 2: cbind(cystfibr$bmp, cystfibr$pemax) ~ age + weight\n  Res.Df Df Gen.var.  Pillai approx F num Df den Df Pr(&gt;F)\n1     18      132.05                                      \n2     22  4   154.48 0.56897   1.7892      8     36 0.1116\n\n# The large p-value 0.1116 provides good evidence that the model with\n# two predictors fits as well as the model with six predictors.\n\n\n#------------------------------------------------------------------------#\n\n# Question B.\n\n# The secher data(\"secher\") are best analyzed after log-transforming birth\n# weight as well as the abdominal and biparietal diameters. Fit a prediction\n# weight as well as abdominal and biparietal diameters.\ndata(\"secher\")\nattach(secher)\n# Fit linear regression for 'bwt' using the log transformed 'ad'\n# Model with only abdominal diameter:\nmodel_ad&lt;-lm(log(bwt)~I(log(ad)),data=secher)\nsummary(model_ad)\n\n\nCall:\nlm(formula = log(bwt) ~ I(log(ad)), data = secher)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58560 -0.06609  0.00184  0.07479  0.48435 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -2.4446     0.5103  -4.791 5.49e-06 ***\nI(log(ad))    2.2365     0.1105  20.238  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1275 on 105 degrees of freedom\nMultiple R-squared:  0.7959,    Adjusted R-squared:  0.794 \nF-statistic: 409.6 on 1 and 105 DF,  p-value: &lt; 2.2e-16\n\n# Fit linear regression for 'bwt' using the log transformed 'bpd'\n# Model with only biparietal diameter:\nmodel_bpd&lt;-lm(log(bwt)~I(log(bpd)),data=secher)\nsummary(model_bpd)\n\n\nCall:\nlm(formula = log(bwt) ~ I(log(bpd)), data = secher)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.36478 -0.09725  0.01251  0.07703  0.51154 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -7.0862     0.9062  -7.819 4.35e-12 ***\nI(log(bpd))   3.3320     0.2017  16.516  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1488 on 105 degrees of freedom\nMultiple R-squared:  0.7221,    Adjusted R-squared:  0.7194 \nF-statistic: 272.8 on 1 and 105 DF,  p-value: &lt; 2.2e-16\n\n# Fit linear regression for 'bwt' using the log transformed 'ad' + 'bpd'\n# Combine both models:\nmodel_combined&lt;-lm(log(bwt)~I(log(ad))+I(log(bpd)),data=secher)\nsummary(model_combined)\n\n\nCall:\nlm(formula = log(bwt) ~ I(log(ad)) + I(log(bpd)), data = secher)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.35074 -0.06741 -0.00792  0.05750  0.36360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -5.8615     0.6617  -8.859 2.36e-14 ***\nI(log(ad))    1.4667     0.1467   9.998  &lt; 2e-16 ***\nI(log(bpd))   1.5519     0.2294   6.764 8.09e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1068 on 104 degrees of freedom\nMultiple R-squared:  0.8583,    Adjusted R-squared:  0.8556 \nF-statistic: 314.9 on 2 and 104 DF,  p-value: &lt; 2.2e-16\n\n# For a prediction equation for birth weight.\n\n# birthweight = -5.8615 + 1.4667(abdominal) + 1.5519(biparietal)\n\n# B1. How much is gained by using both diameters in a prediction equation?\nr_sq_model_ad&lt;-summary(model_ad)$r.squared\nr_sq_model_bpd&lt;-summary(model_bpd)$r.squared\nr_sq_model_combined&lt;-summary(model_combined)$r.squared\nr_sq_gain&lt;-r_sq_model_combined - mean(c(r_sq_model_ad,r_sq_model_bpd))\ncat(\"The Avg. Gain in R-sq is approx:\", r_sq_gain, \"\\n\")\n\nThe Avg. Gain in R-sq is approx: 0.09928108 \n\n# The sum of the two regression coefficients is almost identical and \n# equal to 3.\n\n# B2. Can this be given a nice interpretation to our analysis?\n# Please provide step by step on your analysis and code you use\n# to find out the result. \n\n# The regression coefficients for log(ad) and log(bpd) are approx.(1.5) each.\n# Their sum is approx. (3).\ncat(\"Regression coefficients:\",\"\\n\",\n\"Abdominal diameter:\", coef(model_combined)[2],\"\\n\",\n\"Biparietal diameter:\", coef(model_combined)[3],\"\\n\",\n\"Sum of coefficients:\", coef(model_combined)[2] + coef(model_combined)[3],\"\\n\")\n\nRegression coefficients: \n Abdominal diameter: 1.466662 \n Biparietal diameter: 1.551943 \n Sum of coefficients: 3.018605 \n\n# This suggests that a 1% increase in both abdominal and biparietal diameters \n# is associated with an approximate 3% increase in birth weight.\n\n# Model without log-transformation:\nmodel_reg&lt;-lm(bwt~ad+bpd,data=secher)\n# Predict 'bwt' for 1% increase in both 'ad' & 'bpd'\npredict(model_reg,data.frame(ad=c(95,96),bpd=c(95,96)))\n\n       1        2 \n2676.999 2753.895 \n\n# Calculate percent increase\npercentIncrease&lt;-((2753.895-2676.999)/2676.999)*100\ncat(\"For a 1% increase from 95mm to 96mm in both 'ad' & 'bpd'\nThe 'bwt' gain in grams is approx:\", percentIncrease, \"%\")\n\nFor a 1% increase from 95mm to 96mm in both 'ad' & 'bpd'\nThe 'bwt' gain in grams is approx: 2.87247 %\n\n# B3. Just an additional question (This will not be graded).\n# When should we consider \"log-transforming\" a dataset?\n# This is a very common practice in data science. \n\ncat(\"When simplifying complex relationships. For example,\nsome relationships between variables might be difficult to interpret \nin the original scale. Log-transforming could simplify the complex\nrelationships, making them easier to understand and model.\")\n\nWhen simplifying complex relationships. For example,\nsome relationships between variables might be difficult to interpret \nin the original scale. Log-transforming could simplify the complex\nrelationships, making them easier to understand and model.\n\n# References:\n\nFox, J and Weisberg, S (2011). An {R} Companion to Applied Regression, Second Edition. Thousand Oaks CA: Sage. URL: http://socserv.socsci.mcmaster.ca/jfox/Books/Companion\nFord, C (2024). Getting started with Multivariate Multiple Regression with R blog posting. Hosted by the University of Virginia Library."
  },
  {
    "objectID": "posts/Module 12 Assignment/index.html",
    "href": "posts/Module 12 Assignment/index.html",
    "title": "Module 12 Assignment",
    "section": "",
    "text": "Exercise in Time Series Regression!\n\n# LIS4273 - Module 12. Assignment\n# Robert Walsh\n# Professor Lingyao Li\n\n# Question A.\n\n# The table below represents charges for a student credit card. \n\n\n\n# A1. Construct a time series plot using R.\n\n# Enter data:\n\ncharge&lt;-c(31.9,27,31.3,31,39.4,40.7,42.3,49.5,45,50,50.9,58.5,39.4,36.2,40.5,44.6,46.8,44.7,52.2,54,48.8 ,55.8,58.7,63.4)\nchargeTimeSeries&lt;-ts(charge,frequency=12,start=c(2012,1))\nchargeTimeSeries\n\n      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec\n2012 31.9 27.0 31.3 31.0 39.4 40.7 42.3 49.5 45.0 50.0 50.9 58.5\n2013 39.4 36.2 40.5 44.6 46.8 44.7 52.2 54.0 48.8 55.8 58.7 63.4\n\n# Plot the time series regression:\nplot.ts(chargeTimeSeries)\n\n\n\n\n\n\n\n# A2. Employ the Exponential Smoothing Model as outlined in Avril Voghlan's\n#     notes and report the statistical  outcome:\nchargeTimeSeriesForecasts&lt;-HoltWinters(chargeTimeSeries)\nchargeTimeSeriesForecasts\n\nHolt-Winters exponential smoothing with trend and additive seasonal component.\n\nCall:\nHoltWinters(x = chargeTimeSeries)\n\nSmoothing parameters:\n alpha: 0.4786973\n beta : 0\n gamma: 0.1\n\nCoefficients:\n           [,1]\na    51.4481469\nb     0.6088578\ns1   -6.6831338\ns2  -10.5867440\ns3   -6.6998393\ns4   -3.0320795\ns5   -1.4068647\ns6   -4.0422184\ns7    0.4727766\ns8    6.6378768\ns9    1.4431586\ns10   5.6809745\ns11   5.7999737\ns12  12.6976853\n\n# The output of HoltWinters() estimates the alpha @ 0.4786973\n# Close to .50, suggesting that the forecasts are based on both\n# the past and future observations almost equal weight.\n\n# Plot with Exponential Smoothing:\nplot(chargeTimeSeriesForecasts)\n\n\n\n\n\n\n\n# The original time series in black, and the red line is the forecast.\n# The time series regression of forecasts is smoother.\n\n\n# The distance between the fitted value and the actual observed value.\n# Using the measure of accuracy, or sum of squared errors:\nchargeTimeSeriesForecasts$SSE\n\n[1] 62.88946\n\n# A3. Provide a discussion on the time series and Exponential Smoothing\n#     Model result you led to. \n\n# This time series could be described using an additive model,\n# the seasonal and random fluctuations seem to be roughly constant\n# in size over time with an overall positive trend including\n# some annual seasonal increase/decrease trends as well:\n\n# The students spend the least at the beginning of the year,\n# a significant increase in the middle of the year,\n# and spend the most towards the end of the year.\n# There is a sharp decline in spending at the beginning of each new year.\n\n# To learn more about the component trends of the data set, the \n# decompose() function can be used:\nchargeTimeSeriesComponents&lt;-decompose(chargeTimeSeries)\nplot(chargeTimeSeriesComponents)\n\n\n\n\n\n\n\n# The top chart shows the observed data.\n\n# The trend shows an increase from 41.77083 in July 2012 to\n# 48.55417 in June of 2013.\n\n# The \"seasonal\" components:\nchargeTimeSeriesComponents$seasonal\n\n             Jan         Feb         Mar         Apr         May         Jun\n2012  -6.8986111 -10.6986111  -6.7444444  -3.0444444  -1.4111111  -4.0402778\n2013  -6.8986111 -10.6986111  -6.7444444  -3.0444444  -1.4111111  -4.0402778\n             Jul         Aug         Sep         Oct         Nov         Dec\n2012   0.3430556   6.8472222   1.5805556   5.6305556   5.6555556  12.7805556\n2013   0.3430556   6.8472222   1.5805556   5.6305556   5.6555556  12.7805556\n\n# These figures confirm the sharp -6.8986111 to -10.6986111 decline from\n# Jan. into Feb., the 6.8472222 increase in Aug. and the 12.7805556\n# increase in Dec.\n\n# Using simple exponential smoothing to make short-term future forecasts.\nlibrary(\"forecast\")\nchargeFutureForecast &lt;- forecast(chargeTimeSeriesForecasts,h=12)\nplot(chargeFutureForecast)\n\n\n\n\n\n\n\n# This plot displays the prediction for 2014."
  },
  {
    "objectID": "posts/Module 3 Assignment/index.html",
    "href": "posts/Module 3 Assignment/index.html",
    "title": "Module 3 Assignment",
    "section": "",
    "text": "An exercise in descriptive statistics!\nThis was a lot of fun!\n\n# LIS4273 - Module 3. Assignment\n# Robert Walsh\n# Professor Lingyao Li\n#\nlibrary(DescTools)\n# Import the two data sets from .csv file\ndataSets&lt;-read.csv(\"C:/LIS4273blog/Mod3DataSets.csv\",header = TRUE,sep = \",\")\ndataSets\n\n  Set.1 Set.2\n1    10    20\n2     2    12\n3     3    13\n4     2    12\n5     4    14\n6     2    12\n7     5    15\n\n# (A1) For each set, compute the mean, median, \n# and mode under Central Tendency\n#\n# ----------------\n# CENTRAL TENDENCY\n# ----------------\n# Set # 1, Mean:\nmean(dataSets$Set.1)\n\n[1] 4\n\n# Set # 1, Median:\nmedian(dataSets$Set.1)\n\n[1] 3\n\n# Set # 1, Mode:\nMode(dataSets$Set.1)\n\n[1] 2\nattr(,\"freq\")\n[1] 3\n\n# ----------------\n# CENTRAL TENDENCY\n# ----------------\n# Set # 2 Mean:\nmean(dataSets$Set.2)\n\n[1] 14\n\n# Set # 2 Median:\nmedian(dataSets$Set.2)\n\n[1] 13\n\n# Set # 2 Mode:\nMode(dataSets$Set.2)\n\n[1] 12\nattr(,\"freq\")\n[1] 3\n\n# (A2) For each set, compute the range, interquartile, \n# variance, and standard deviation under Variation\n#\n# ---------\n# VARIATION\n# ---------\nset1_minMax&lt;-range(dataSets$Set.1)\n# The Min & Max of Set # 1 are:\nset1_minMax\n\n[1]  2 10\n\nset1_range&lt;-set1_minMax[2]-set1_minMax[1]\n# The Range of Set # 1 is:\nset1_range\n\n[1] 8\n\nquartileSet1&lt;-quantile(dataSets$Set.1)\n# The Quartiles of Set # 1 are:\nquartileSet1\n\n  0%  25%  50%  75% 100% \n 2.0  2.0  3.0  4.5 10.0 \n\ninterQuartileRangeSet1&lt;-as.numeric(quartileSet1[4])-as.numeric(quartileSet1[2])\n# The InterQuartile Range for Set # 1 is:\ninterQuartileRangeSet1\n\n[1] 2.5\n\n# The Variance of Set # 1 is:\nvar(dataSets$Set.1)\n\n[1] 8.333333\n\n# The Standard Deviation of Set # 1 is:\nsd(dataSets$Set.1)\n\n[1] 2.886751\n\n# ---------\n# VARIATION\n# ---------\nset2_minMax&lt;-range(dataSets$Set.2)\n# The Min & Max of Set # 2 are:\nset2_minMax\n\n[1] 12 20\n\nset2_range&lt;-set2_minMax[2]-set2_minMax[1]\n# The Range of Set # 2 is:\nset1_range\n\n[1] 8\n\nquartileSet2&lt;-quantile(dataSets$Set.2)\n# The Quartiles of Set # 2 are:\nquartileSet2\n\n  0%  25%  50%  75% 100% \n12.0 12.0 13.0 14.5 20.0 \n\ninterQuartileRangeSet2&lt;-as.numeric(quartileSet2[4])-as.numeric(quartileSet2[2])\n# The InterQuartile Range for Set # 2 is:\ninterQuartileRangeSet2\n\n[1] 2.5\n\n# The Variance of Set # 2 is:\nvar(dataSets$Set.2)\n\n[1] 8.333333\n\n# The Standard Deviation of Set # 2 is:\nsd(dataSets$Set.2)\n\n[1] 2.886751\n\n# (A3) Compare your results between Set # 1 vs. Set # 2\n# by discussing the differences between the two sets.\n#\n# --------------------\n# SUMMARY / COMPARISON\n# --------------------\n#\ncv1&lt;-(sd(dataSets$Set.1)/mean(dataSets$Set.1))*100\ncv2&lt;-(sd(dataSets$Set.2)/mean(dataSets$Set.2))*100\ncompdf&lt;-data.frame(\"Mean\"=c(mean(dataSets$Set.1),mean(dataSets$Set.2)),\n\"Median\"=c(median(dataSets$Set.1),median(dataSets$Set.2)),\n\"Mode\"=c(Mode(dataSets$Set.1),Mode(dataSets$Set.2)),\n\"Range\"=c(set1_range,set2_range),\n\"Variance\"=c(var(dataSets$Set.1),var(dataSets$Set.2)),\n\"Std Dev\"=c(sd(dataSets$Set.1),sd(dataSets$Set.2)),\n\"CV\"=c(cv1,cv2),\nrow.names = c(\"Set.1\",\"Set.2\"))\ncompdf\n\n      Mean Median Mode Range Variance  Std.Dev       CV\nSet.1    4      3    2     8 8.333333 2.886751 72.16878\nSet.2   14     13   12     8 8.333333 2.886751 20.61965\n\nsummary(dataSets$Set.1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    2.0     2.0     3.0     4.0     4.5    10.0 \n\nsummary(dataSets$Set.2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    12.0    13.0    14.0    14.5    20.0 \n\n# ------------------------\n# COEFFICIENT OF VARIATION\n# ------------------------\n# Coefficient of variation, as percent:\n# Set # 1:\ncv1\n\n[1] 72.16878\n\n# Set # 2:\ncv2\n\n[1] 20.61965\n\n# Although both sets had an equal range and variance...\n# They had significantly different means.\n# The set of data that has the greatest spread of variance\n# relative to the mean is:\nif (cv1&gt;cv2) {paste(\"Set # 1, Coefficient of Variance = \", cv1, \"VS.\", cv2)\n} else {paste(\"Set # 2, Coefficient of Variance = \", cv2, \"VS.\", cv1)}\n\n[1] \"Set # 1, Coefficient of Variance =  72.1687836487032 VS. 20.6196524710581\""
  },
  {
    "objectID": "posts/Module 5 Assignment/index.html",
    "href": "posts/Module 5 Assignment/index.html",
    "title": "Module 5 Assignment",
    "section": "",
    "text": "An exercise in Hypothesis Testing & Correlation!\n\n# LIS4273 - Module 5. Assignment\n# Robert Walsh\n# Professor Lingyao Li\n\n# Question A.\n\n# The director of manufacturing at a cookies company needs to determine\n# whether a new machine is able to produce a particular type of cookies\n# according to the manufacturer's specifications, which indicate that\n# cookies should have a mean of 70 and standard deviation of 3.5 pounds.\n# A sample of 49 cookies reveals a sample mean breaking strength\n# of 69.1 pounds.\n\n# A1. State the null and alternative hypothesis :\n\n# Null Hypothesis - Ho: The mean µ = 70\n# New machine is able to produce cookies within spec.\n# Mean breaking strength is equal to 70\n\n# Alt Hypothesis - Ha: The mean µ ≠ 70\n# New machine is not able to produce cookies within spec.\n# Mean breaking strength is not equal to 70\n\n# A2. Is there evidence that the machine is not meeting the manufacturer's\n# specifications for average strength?\n\n# Use a 0.05 level of significance.\nalpha &lt;- 0.05\n# Population Mean = 70\na &lt;- 70\n# Standard Deviation = 3.5\ns &lt;- 3.5\n# Sample Size = 49\nn &lt;- 49\n# Sample Mean = 69.1\nxbar &lt;- 69.1\n# Calculate Test Statistic:\nz &lt;- (xbar-a)/(s/sqrt(n))\nz\n\n[1] -1.8\n\n# Use qnorm() to determine critical values @ 0.05 alpha\n# Two tailed test would identify a 0.025 rejection area on each tail\nz.half.alpha &lt;- qnorm(1-alpha/2)\n# The critical vales are - 1.96 and + 1.96\nz.half.alpha\n\n[1] 1.959964\n\n-z.half.alpha\n\n[1] -1.959964\n\n# Determine if the Test Statistic is in the rejection region.\nz &lt; -z.half.alpha || abs(z) &gt; z.half.alpha\n\n[1] FALSE\n\n# -1.96 &gt; -1.80 &gt; 0 &lt; 1.80 &lt; 1.96\n# The Test Statistic falls between the critical values in the\n# 'do not reject Ho/Null' region. Therefore we can infer\n# that there is evidence that the machine is meeting the manufacturer’s \n# specifications for average strength.\n\n# A3. Compute the p value and interpret its meaning.\n\n# Use pnorm() to calculate the p value using the Test Statistic 'Z'\n# Then multiply the value by two for the two tailed test.\np &lt;- 2*pnorm(z)\n# We get a p value of:\np\n\n[1] 0.07186064\n\n# Test to see if this p value is greater than alpha.\np &gt; alpha\n\n[1] TRUE\n\n# Because the p value is greater than alpha, there is not enough\n# evidence to reject the Null hypothesis - Ho.\n\n# A4. What would be your answer in (A2) if the standard\n# deviation were specified as 1.75 pounds?\n\nalpha &lt;- 0.05\n# Population Mean = 70\na &lt;- 70\n# Standard Deviation = 1.75\ns &lt;- 1.75\n# Sample Size = 49\nn &lt;- 49\n# Sample Mean = 69.1\nxbar &lt;- 69.1\n# Calculate Test Statistic:\nz &lt;- (xbar-a)/(s/sqrt(n))\nz\n\n[1] -3.6\n\n# Use qnorm() to determine critical values @ 0.05 alpha\n# Two tailed test would identify a 0.025 rejection area on each tail\nz.half.alpha &lt;- qnorm(1-alpha/2)\n# The critical vales are - 1.96 and + 1.96\nz.half.alpha\n\n[1] 1.959964\n\n-z.half.alpha\n\n[1] -1.959964\n\n# Determine if the Test Statistic is in the rejection region.\nz &lt; -z.half.alpha || abs(z) &gt; z.half.alpha\n\n[1] TRUE\n\n# -3.60 &gt; -1.96 &gt; 0 &lt; 1.96 &lt; 3.60\n# The Test Statistic falls outside the critical values in the\n# 'reject Ho/Null' region. Therefore we can infer\n# that there is evidence that the machine is not meeting the \n# manufacturer’s specifications for average strength.\n\n# A5. What would be your answer in (A2) if the sample mean were \n# 69 pounds and the standard deviation is 3.5 pounds?\n\nalpha &lt;- 0.05\n# Population Mean = 70\na &lt;- 70\n# Standard Deviation = 3.5\ns &lt;- 3.5\n# Sample Size = 49\nn &lt;- 49\n# Sample Mean = 69\nxbar &lt;- 69\n# Calculate Test Statistic:\nz &lt;- (xbar-a)/(s/sqrt(n))\nz\n\n[1] -2\n\n# Use qnorm() to determine critical values @ 0.05 alpha\n# Two tailed test would identify a 0.025 rejection area on each tail\nz.half.alpha &lt;- qnorm(1-alpha/2)\n# The critical vales are - 1.96 and + 1.96\nz.half.alpha\n\n[1] 1.959964\n\n-z.half.alpha\n\n[1] -1.959964\n\n# Determine if the Test Statistic is in the rejection region.\nz &lt; -z.half.alpha || abs(z) &gt; z.half.alpha\n\n[1] TRUE\n\n# -2 &gt; -1.96 &gt; 0 &lt; 1.96 &lt; 2\n# The Test Statistic falls outside the critical values in the\n# 'reject Ho/Null' region. Therefore we can infer\n# that there is evidence that the machine is not meeting the \n# manufacturer’s specifications for average strength.\n\n\n# Question B. If x̅ = 85, σ = standard deviation = 8, and n=64,\n# set up 95% confidence interval estimate of the population mean μ.\n\n# The 'Zc' Critical Value for a 95% Level of Confidence C = 1.96\nzc &lt;- 1.96\n# Sample Mean = 85\nxbar &lt;- 85\n# Standard Deviation = 8\ns &lt;- 8\n# Sample Size = 64\nn &lt;- 64\n\n# Calculate Upper Confidence Limit value\nxBarPlusE &lt;- xbar + (zc*(s/sqrt(n)))\n# Calculate Upper Confidence Limit value\nxBarMinusE &lt;- xbar - (zc*(s/sqrt(n)))\n# Therefore the 95% confidence interval would be:\nprint(\"(83.04,86.96)\")\n\n[1] \"(83.04,86.96)\"\n\n# There is a 95% probability that the population mean\n# would be in between these two values.\n\n\n# Question C. Given the time spent on assignments each week\n# for the sampled girl group and boy group, respectively,\nlibrary(GGally)\n\nLoading required package: ggplot2\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\ngirls_grades &lt;- c(89, 90, 91, 95, 98, 99, 96, 99)\ngirls_time_spend &lt;- c(19, 20, 22, 25, 28, 30, 32, 36)\n\nboys_grades &lt;- c(86, 84, 92, 93, 93, 96, 98, 98)\nboys_time_spend &lt;- c(15, 19, 22, 23, 25, 29, 30, 40)\n\n# Please perform the correlation analysis:\n\n# C1. Calculate the correlation coefficient (Pearson) between\n# time spent and grade for girls' and boys' datasets, respectively.\n\n# The Pearson correlation coefficient between time spent and grade\n# for the Girls' dataset:\nx&lt;-c(girls_grades)\ny&lt;-c(girls_time_spend)\ngirlsData&lt;-data.frame(x,y)\nround(cor(girlsData,method=\"pearson\"),digits=2)\n\n     x    y\nx 1.00 0.91\ny 0.91 1.00\n\n# This result is close to 1 so a strong positive relation is implied.\n\n# The Pearson correlation coefficient between time spent and grade\n# for the Boys' dataset:\nx&lt;-c(boys_grades)\ny&lt;-c(boys_time_spend)\nboysData&lt;-data.frame(x,y)\nround(cor(boysData,method=\"pearson\"),digits=2)\n\n     x    y\nx 1.00 0.86\ny 0.86 1.00\n\n# This result is close to 1 so a strong positive relation is implied.\n\n# C2. Use ggpairs to plot the time spent and grade for\n# girls' and boys' datasets, respectively.\n\n# For the Girls' dataset:\nggpairs(\n  girlsData,\n  upper = list(wrap(\"cor\", size = 5)),\n  diag = list(continuous = \"densityDiag\"),\n)\n\n\n\n\n\n\n\n# For the Boys' dataset:\nggpairs(\n  boysData,\n  upper = list(wrap(\"cor\", size = 5)), \n  diag = list(continuous = \"densityDiag\"),\n)"
  },
  {
    "objectID": "posts/Module 7 Assignment/index.html",
    "href": "posts/Module 7 Assignment/index.html",
    "title": "Module 7 Assignment",
    "section": "",
    "text": "An Exercise in Simple and Multiple Regression!\n\n# LIS4273 - Module 7. Assignment\n# Robert Walsh\n# Professor Lingyao Li\n\n# Question A.\n\n# In this assignment's segment, we will use the following regression\n# equation  Y = a + bX +e\n\n# Y is the value of the dependent variable (Y),\n# what is being predicted or explained\n\n# a or Alpha, a constant; equals the value of Y when the value of X=0\n\n# b or Beta, the coefficient of X; the slope of the regression line;\n# how much Y changes for each one-unit change in X.\n\n# X is the value of the Independent variable (X),\n# what is predicting or explaining the value of Y\n\n# e is the error term; the error in predicting the value of Y,\n# given the value of X (it is not displayed in most regression equations).\n\n# The data in this assignment:\nx &lt;- c(16, 17, 13, 18, 12, 14, 19, 11, 11, 10)\ny &lt;- c(63, 81, 56, 91, 47, 57, 76, 72, 62, 48)\n\n# A1. Define the relationship model between the independent\n# and the dependent variable.\n\n# 'x' is the value of the Independent variable (X),\n# what is predicting or explaining the value of 'y'.\n# 'y' is the value of the dependent variable (Y),\n# what is being predicted or explained.\n\n# To model the relationship between how one variable affects the other:\nmodel.x.y&lt;-lm(y~x,)\n# Create 'model.x.y' object to store results of.....\n# lm(Y~X), linear model: 'y' is predicted by 'x'\n\n# A2. Calculate the coefficients:\n# Print 'model.x.y' object.\nmodel.x.y\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n     19.206        3.269  \n\n# Question B.\n\n# The following question is posted by Chi Yau (Links to an external site.)\n# the author of  R Tutorial With Bayesian Statistics Using Stan\n# (Links to an external site.) and his blog posting regarding\n# Regression analysis (Links to an external site.).\n\n# Apply the simple linear regression model (see the above formula)\n# for the data set called \"visit\" (see below), and estimate the\n# discharge duration if the waiting time since the last eruption\n# has been 80 minutes.\n\n#  &gt;head(visit) \n#  discharge  waiting \n# 1     3.600      79 \n# 2     1.800      54 \n# 3     3.333      74 \n# 4     2.283      62 \n# 5     4.533      85 \n# 6     2.883      55\n\nvisit&lt;-data.frame(\"discharge\"=c(3.600,1.800,3.333,2.283,4.533,2.883),\n                  \"waiting\"=c(79,54,74,62,85,55))\n\n# Employ the following formula lm(discharge~waiting,data=visit)\n\n# B1. Define the relationship model between the predictor\n# and the response variable.\n\n# The 'predictor' variable predicts the 'response' variable.\n# 'waiting' is the value of the Independent variable (X),\n# what is predicting or explaining the value of 'discharge'.\n# 'discharge' is the value of the dependent variable (Y),\n# what is being predicted or explained.\n\n# To model the relationship between how one variable affects the other:\ndischarge.lm&lt;-lm(discharge~waiting,data=visit)\n# Create 'discharge.lm' object to store results of.....\n# lm(discharge~waiting,data=visit), linear model:\n# 'discharge' is predicted by 'waiting'\n\n# B2. Extract the parameters of the estimated regression equation\n# with the coefficients function.\ncoeffs&lt;-coefficients(discharge.lm)\ncoeffs\n\n(Intercept)     waiting \n-1.53317418  0.06755757 \n\n# B3. Determine the fit of the eruption duration using\n# the estimated regression equation.\n\n# Create 'waitingTime' object and initialize to value of '80'.\nwaitingTime&lt;-80\n# Fit the discharge duration using the estimated regression equation.\ndischargeDuration&lt;-coeffs[1]+coeffs[2]*waitingTime\ndischargeDuration\n\n(Intercept) \n   3.871431 \n\n# Based on the linear regression model, if the waiting time since the\n# last discharge has been 80 minutes, we expect the next discharge duration\n# to last approximately 3.871431 minutes.\n\n# Question C.  Multiple regression\n\n# We will use a very famous datasets in R called mtcars. This dateset\n# was extracted from the 1974 Motor Trend US magazine, and comprises\n# fuel consumption and 10 aspects of automobile design and performance\n# for 32 automobiles (1973--74 models).\n\n# This data frame contain 32 observations on 11 (numeric) variables.\n\n# [, 1] mpg Miles/(US) gallon\n# [, 2] cyl Number of cylinders\n# [, 3] disp    Displacement (cu.in.)\n# [, 4] hp  Gross horsepower\n# [, 5] drat    Rear axle ratio\n# [, 6] wt  Weight (1000 lbs)\n# [, 7] qsec    1/4 mile time\n# [, 8] vs  Engine (0 = V-shaped, 1 = straight)\n# [, 9] am  Transmission (0 = automatic, 1 = manual)\n# [,10] gear    Number of forward gears\n\n# To call mtcars data in R\n# R comes with several built-in data sets, which are generally used\n# as demo data for playing with R functions. One of those datasets\n# build in R is mtcars.\n\n# In this question, we will use 4 of the variables found in mtcars\n# by using the following function\n# input &lt;- mtcars[,c(\"mpg\",\"disp\",\"hp\",\"wt\")]\n\n# C1. Examine the relationship Multi Regression Model as stated above\n# and its Coefficients using 4 different variables from mtcars\n# (mpg, disp, hp and wt).\n# Report on the result and explanation what does the multi regression\n# model and coefficients tell about the data.\ninput &lt;- mtcars[,c(\"mpg\",\"disp\",\"hp\",\"wt\")]\nprint(head(input))\n\n                   mpg disp  hp    wt\nMazda RX4         21.0  160 110 2.620\nMazda RX4 Wag     21.0  160 110 2.875\nDatsun 710        22.8  108  93 2.320\nHornet 4 Drive    21.4  258 110 3.215\nHornet Sportabout 18.7  360 175 3.440\nValiant           18.1  225 105 3.460\n\nlm(formula=mpg~disp+hp+wt,data=input)\n\n\nCall:\nlm(formula = mpg ~ disp + hp + wt, data = input)\n\nCoefficients:\n(Intercept)         disp           hp           wt  \n  37.105505    -0.000937    -0.031157    -3.800891  \n\ncat(\"The relationship multi regression model demonstrates that 'mpg' is \nthe 'response variable' being predicted by 'disp', 'hp', and 'wt' which\nare the 'predictor variables.' What is learned from the coefficients is that\n'disp' and 'hp' have coefficients close to 0 which means that they may relate\nin similar ways. The -3.8 coefficient for 'wt' means that as 'wt' increases,\n'mpg' decreases and as 'mpg' increases, 'wt' decreases. This demonstrates how\neach variable predicts or affects 'mpg' differently and to what magnitude. Each\npredictor variable is individually related to the response variable.\")\n\nThe relationship multi regression model demonstrates that 'mpg' is \nthe 'response variable' being predicted by 'disp', 'hp', and 'wt' which\nare the 'predictor variables.' What is learned from the coefficients is that\n'disp' and 'hp' have coefficients close to 0 which means that they may relate\nin similar ways. The -3.8 coefficient for 'wt' means that as 'wt' increases,\n'mpg' decreases and as 'mpg' increases, 'wt' decreases. This demonstrates how\neach variable predicts or affects 'mpg' differently and to what magnitude. Each\npredictor variable is individually related to the response variable."
  },
  {
    "objectID": "posts/Module 9 Assignment/index.html",
    "href": "posts/Module 9 Assignment/index.html",
    "title": "Module 9 Assignment",
    "section": "",
    "text": "An Exercise in tabular data!\n\n# LIS4273 - Module 9. Assignment\n# Robert Walsh\n# Professor Lingyao Li\n\n# Question A.\n\n# Your data.frame is listed as below.\n\ndf &lt;- data.frame(\n  country = c(\"France\", \"Spain\", \"Germany\", \"Spain\", \"Germany\", \"France\",\n              \"Spain\", \"France\", \"Germany\", \"France\"),\n  age = c(44, 27, 30, 38, 40, 35, 52, 48, 45, 37),\n  salary = c(6000, 5000, 7000, 4000, 8000, 6000, 5000, 7000, 4000, 8000),\n  purchased = c(\"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"Yes\", \"No\",\n                \"Yes\", \"No\", \"Yes\")\n)\n# Please do the following:\n\n# A1. Generate a one-way table for \"purchased\"\noneWayPurchased&lt;-table(df$purchased)\noneWayPurchased\n\n\n No Yes \n  5   5 \n\n# A2. Generate a two-way table for \"country\" and \"purchased.\"\ntwoWayCountryPurchased&lt;-table(df$country,df$purchased)\ntwoWayCountryPurchased\n\n         \n          No Yes\n  France   1   3\n  Germany  2   1\n  Spain    2   1\n\n# Question B.\n\n# Generate contingency table also known as rx C table using mtcars dataset. \ndata(mtcars)\nmtcars_df &lt;- table(mtcars$gear, mtcars$cyl, dnn = c(\"Gears\", \"Cylinders\"))\nmtcars_df\n\n     Cylinders\nGears  4  6  8\n    3  1  2 12\n    4  8  4  0\n    5  2  1  2\n\n# B1. Add the addmargins() function to report on the sum totals of the\n#     rows and columns of \"mtcars_df\" table\naddmargins(mtcars_df)\n\n     Cylinders\nGears  4  6  8 Sum\n  3    1  2 12  15\n  4    8  4  0  12\n  5    2  1  2   5\n  Sum 11  7 14  32\n\n# addmargins() function:\n# Displays the margins of the table, or the sum of each row and/or column\n# to help understand differences among factors.\n# There are more three gear cars than four or five gear cars and more eight\n# cylinder cars than four or six cylinder cars.\n\n# B2. Add prop.tables() function, and report on the proportional weight\n#     of each value in the \"mtcars_df\" table\nprop.table(mtcars_df)\n\n     Cylinders\nGears       4       6       8\n    3 0.03125 0.06250 0.37500\n    4 0.25000 0.12500 0.00000\n    5 0.06250 0.03125 0.06250\n\n# prop.table() function:\n# Displays a table of proportions rather than sums. The default argument for\n# 'margin=', the sum of all cells in the table will equal 1.\n# Therefore the largest proportion of cars have eight cylinders and \n# three gears, 37.5%, and 0% of the cars have eight cylinders and four gears."
  }
]